{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13909562,"sourceType":"datasetVersion","datasetId":8862641}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 0 ‚Äì FIXED DEPENDENCIES (Run this first ‚Üí then RESTART SESSION)\n!pip install -q --no-deps bitsandbytes transformers accelerate peft\n!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install -q --no-deps trl==0.24.0\n!pip install -q portalocker sacrebleu rouge_score\n\nprint(\"Dependencies installed! Click 'Restart Session' now, then run the next cells.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 1 ‚Äì Imports & Full OOP + Strategy Pattern (FIXED)\nimport torch\nimport pandas as pd\nimport json\nfrom datetime import datetime\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom datasets import load_dataset\nfrom rouge_score import rouge_scorer\nimport sacrebleu\nimport math\n\n# Strategy Pattern\nclass FineTuningStrategy:\n    def apply(self, model, tokenizer, dataset, args): \n        pass\n\nclass UnslothStrategy(FineTuningStrategy):\n    def apply(self, model, tokenizer, dataset, args):\n        return SFTTrainer(\n            model=model,\n            tokenizer=tokenizer,\n            train_dataset=dataset[\"train\"],\n            eval_dataset=dataset[\"validation\"],\n            dataset_text_field=\"text\",\n            max_seq_length=4096,\n            args=args,\n            packing=False,\n        )\n\n# Dataset Processor\nclass DatasetProcessor:\n    def preprocess(self, path):\n        df = pd.read_csv(path, encoding='utf-8-sig')\n        print(f\"Loaded {len(df)} Bengali conversations\")\n        df['text'] = \"User: \" + df['Questions'].astype(str) + \"\\nAssistant: \" + df['Answers'].astype(str)\n        train = df.sample(frac=0.9, random_state=42)\n        val = df.drop(train.index)\n        train[['text']].to_json(\"train.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n        val[['text']].to_json(\"val.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n        return load_dataset(\"json\", data_files={\"train\": \"train.jsonl\", \"validation\": \"val.jsonl\"}), val\n\n# Fine-Tuner Class (Fixed __init__)\nclass LLAMAFineTuner:\n    def __init__(self, strategy):  # ‚Üê FIXED: __init__ not **init**\n        self.strategy = strategy\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n            max_seq_length=4096,\n            load_in_4bit=True,\n            dtype=None,  # Auto detection\n            device_map=\"auto\",\n        )\n        self.model = FastLanguageModel.get_peft_model(\n            model,\n            r=16,\n            lora_alpha=32,\n            target_modules=[\"q_proj\", \"v_proj\"],\n            lora_dropout=0,\n            bias=\"none\",\n            use_gradient_checkpointing=\"unsloth\",\n            random_state=42,\n        )\n        self.tokenizer = tokenizer\n\n    def fine_tune(self, dataset):\n        args = TrainingArguments(\n            per_device_train_batch_size=2,\n            gradient_accumulation_steps=8,\n            warmup_steps=10,\n            max_steps=120,\n            learning_rate=2e-4,\n            fp16=not torch.cuda.is_bf16_supported(),\n            bf16=torch.cuda.is_bf16_supported(),\n            logging_steps=5,\n            output_dir=\"bengali_llama_finetuned\",\n            optim=\"adamw_8bit\",\n            weight_decay=0.01,\n            lr_scheduler_type=\"linear\",\n            report_to=\"none\",\n            save_steps=60,\n            save_total_limit=2,\n            seed=42,\n        )\n        trainer = self.strategy.apply(self.model, self.tokenizer, dataset, args)\n        trainer.train()\n        return trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:01:43.898976Z","iopub.execute_input":"2025-11-29T15:01:43.899171Z","iopub.status.idle":"2025-11-29T15:02:05.072255Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-29 15:01:48.085658: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764428508.107187     211 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764428508.113962     211 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ü¶• Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# CELL 2 ‚Äì Load Dataset\nprocessor = DatasetProcessor()\ndataset, val_df = processor.preprocess(\"/kaggle/input/bengaliempatheticconversationscorpus/BengaliEmpatheticConversationsCorpus .csv\")\nprint(\"Dataset ready ‚Äì starting training in next cell...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:02:31.145077Z","iopub.execute_input":"2025-11-29T15:02:31.145581Z","iopub.status.idle":"2025-11-29T15:02:32.432014Z","shell.execute_reply.started":"2025-11-29T15:02:31.145555Z","shell.execute_reply":"2025-11-29T15:02:32.431276Z"}},"outputs":[{"name":"stdout","text":"Loaded 38233 Bengali conversations\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd8ae64cf7ea411998b08ba90806cb6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68796e90eeda45188d3c07c42559d31f"}},"metadata":{}},{"name":"stdout","text":"Dataset ready ‚Äì starting training in next cell...\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# CELL 3 ‚Äì TRAIN THE MODEL (FULLY FIXED: Transformers 4.57 + Bengali-Optimized)\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\n# Load model with Bengali-safe settings\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    max_seq_length=4096,\n    load_in_4bit=True,\n    dtype=None,  # Auto-detect (fp16/bf16)\n    device_map=\"auto\",\n)\n\n# Apply LoRA with HIGHER CAPACITY for non-English (prevents gibberish)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=64,                          # ‚Üë from 16: More parameters for Bengali nuance\n    target_modules=[               # ‚Üë from just q/v: Train ALL modules for full adaptation\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    lora_alpha=64,                 # ‚Üë from 32: Stronger LoRA signal\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=42,\n)\n\n# Trainer with FIXED Transformers args + longer training\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    dataset_text_field=\"text\",\n    max_seq_length=4096,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=8,\n        warmup_steps=20,               # ‚Üë from 10: Gentler warmup\n        max_steps=300,                 # ‚Üë from 120: Train longer for coherence ( ~5-6 hours on T4x2)\n        learning_rate=2e-4,\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=10,\n        eval_strategy=\"steps\",         # ‚Üê FIXED: eval_strategy (not evaluation_strategy)\n        eval_steps=100,\n        save_steps=150,\n        output_dir=\"bengali_llama_finetuned\",\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"cosine\",    # ‚Üë from linear: Smoother decay for better convergence\n        report_to=\"none\",\n        seed=42,\n    ),\n    packing=False,\n)\n\nprint(\"Starting Bengali-optimized fine-tuning (300 steps)...\")\ntrainer.train()\n\n# Save the merged model\ntrainer.save_model(\"final_model\")\ntokenizer.save_pretrained(\"final_model\")\nprint(\"Training complete! Model saved to 'final_model'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:45:41.777914Z","iopub.execute_input":"2025-11-29T15:45:41.778712Z","iopub.status.idle":"2025-11-29T18:30:49.226135Z","shell.execute_reply.started":"2025-11-29T15:45:41.778686Z","shell.execute_reply":"2025-11-29T18:30:49.225176Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nStarting Bengali-optimized fine-tuning (300 steps)...\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 2\n   \\\\   /|    Num examples = 34,410 | Num Epochs = 1 | Total steps = 300\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n \"-____-\"     Trainable parameters = 167,772,160 of 8,198,033,408 (2.05% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 2:44:14, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.701900</td>\n      <td>0.717911</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.684100</td>\n      <td>0.682775</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.655100</td>\n      <td>0.671973</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"},{"name":"stdout","text":"Training complete! Model saved to 'final_model'\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# CHECK IF YOUR GOOD MODEL IS REALLY THERE\n!ls -la final_model/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T18:36:40.606384Z","iopub.execute_input":"2025-11-29T18:36:40.606700Z","iopub.status.idle":"2025-11-29T18:36:40.886495Z","shell.execute_reply.started":"2025-11-29T18:36:40.606679Z","shell.execute_reply":"2025-11-29T18:36:40.885565Z"}},"outputs":[{"name":"stdout","text":"total 672324\ndrwxr-xr-x 2 root root      4096 Nov 29 15:39 .\ndrwxr-xr-x 7 root root      4096 Nov 29 15:41 ..\n-rw-r--r-- 1 root root      1060 Nov 29 18:30 adapter_config.json\n-rw-r--r-- 1 root root 671149168 Nov 29 18:30 adapter_model.safetensors\n-rw-r--r-- 1 root root      4614 Nov 29 18:30 chat_template.jinja\n-rw-r--r-- 1 root root      5262 Nov 29 18:30 README.md\n-rw-r--r-- 1 root root       454 Nov 29 18:30 special_tokens_map.json\n-rw-r--r-- 1 root root     50641 Nov 29 18:30 tokenizer_config.json\n-rw-r--r-- 1 root root  17209920 Nov 29 18:30 tokenizer.json\n-rw-r--r-- 1 root root      5816 Nov 29 18:30 training_args.bin\n","output_type":"stream"}],"execution_count":10}]}